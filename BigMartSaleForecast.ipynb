{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we load the csv file. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h = pd.read_csv(\"../input/big-mart-sale-forecast/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's inspect how many rows and columns the dataset has. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we inspect the first few entries of the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's examine the data types of variables in the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Item_Fat_Content\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that the LF and low fat refer to Low Fat while Regular and reg both refer to the same fat content. So we fix this. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Item_Fat_Content\"] = h[\"Item_Fat_Content\"].str.replace(\"LF\", \"Low Fat\")\nh[\"Item_Fat_Content\"] = h[\"Item_Fat_Content\"].str.replace(\"low fat\", \"Low Fat\")\nh[\"Item_Fat_Content\"] = h[\"Item_Fat_Content\"].str.replace(\"reg\", \"Regular\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Item_Fat_Content\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Item_Fat_Content', data=h, palette='deep')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that there Low Fat value is common for most items. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Item_Type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.factorplot(\"Item_Type\", data=h, aspect=1.5, kind=\"count\", color=\"r\")\ng.set_xticklabels(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that 'Fruits and Vegetables' and 'Snack Foods' are the most commonly occuring item types. 'Seafood' and 'Breakfast' are the least commonly ocuring item types. "},{"metadata":{},"cell_type":"markdown","source":"Let's find out the number of outlet's that are involved in this dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"len(h[\"Outlet_Identifier\"].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We find out the numerical distribution of the size of the outlets."},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Outlet_Size\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Outlet_Size', data=h, palette='rocket')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's evident from the countplot above that small and medium are sizes of most of the outlets. A minority of the outlets are high sized. "},{"metadata":{},"cell_type":"markdown","source":"Let us find out the numerical proportion of the categorical types in Outlet locations."},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Outlet_Location_Type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Outlet_Location_Type', data=h, palette='Set3')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we concur that the least number of stores are in tier 1 regions, after that the least number of stores are in tier 2 region. Most of the stores are in tier 3 region. "},{"metadata":{"trusted":true},"cell_type":"code","source":"numbers = list(h.select_dtypes(['float64', 'int64']).keys())\nnumbers.remove('Outlet_Establishment_Year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h[numbers].hist(figsize=(20,10), color='green', edgecolor='white')\n\nplt.show()\n\ndisplay(h[numbers].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=h[\"Item_Weight\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that the lightest item is 4.55 and the heaviest item is 21.35. While the middle 50% of the items are from 8.77 to 16.85. The median weight of an item is 12.60.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=h[\"Item_Visibility\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is the plot of *Item_Visibility*. It displays the % of total display area of all products in a store allocated to the particular product. While the least visible products have no visibility at all and the most visible products is 0.33. The middle 50% of the products are between 0.03 and 0.09. The median visibility of an item is 0.05. Most products don't have much visibility. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=h[\"Item_MRP\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is the boxlot of *Item_MRP*. MRP stands for maximum retail price of a item. Cheapest item is worth 31.29 and the most expensive item is worth 266.89. The median worth of an item is 143.01 and the middle 50% of the items are worth 93.84 to 185.65. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Outlet_Size\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that the outlet size is an ordinal variable. We replace Small with 1, Medium with 2 and High with 3. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Outlet_Size\"] = h[\"Outlet_Size\"].replace(\"Medium\", 2)\nh[\"Outlet_Size\"] = h[\"Outlet_Size\"].replace(\"Small\", 1)\nh[\"Outlet_Size\"] = h[\"Outlet_Size\"].replace(\"High\", 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Outlet_Size\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that our attempt has been successful. "},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Outlet_Location_Type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that outlet location type too is an ordinal variable. So we replace Tier 1, Tier 2 and Tier 3 with 1, 2 and 3 respectively. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Outlet_Location_Type\"] = h[\"Outlet_Location_Type\"].replace(\"Tier 1\", 1)\nh[\"Outlet_Location_Type\"] = h[\"Outlet_Location_Type\"].replace(\"Tier 2\", 2)\nh[\"Outlet_Location_Type\"] = h[\"Outlet_Location_Type\"].replace(\"Tier 3\", 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Item_Fat_Content\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that this variable too can be considered an ordinal variable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Item_Fat_Content\"] = h[\"Item_Fat_Content\"].replace(\"Low Fat\", 1)\nh[\"Item_Fat_Content\"] = h[\"Item_Fat_Content\"].replace(\"Regular\", 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h[\"Item_Type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that this variable is a categorical variable with many values. So we perform one hot encoding. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h = pd.concat([h, pd.get_dummies(h.Item_Type, prefix = 'Item_Type') ] , axis = 1)\nh.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having applied one hot encoding to the item type variable. We drop the column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h = h.drop(['Item_Type'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we apply one hot encoding to the outlet type variable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h = pd.concat( [h, pd.get_dummies(h.Outlet_Type, prefix = 'Outlet_Type') ] , axis = 1)\nh.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h = h.drop(['Outlet_Type'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we drop the Item Identifier and Outlet Identifier variables as they are of no use to use in any sense. They are constructs that won't help us in prediction. "},{"metadata":{"trusted":true},"cell_type":"code","source":"h.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h = h.drop(['Item_Identifier', 'Outlet_Identifier'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We finally look at the missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"h.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that two variables *Item_Weight* and *Outlet_Size* have missing values. We use K-nearest neighbours algorithm to cluster datapoints and impute missing values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#u = h[[\"Item_Weight\", \"Item_Visibility\", \"Item_MRP\", \"Outlet_Establishment_Year\", \"Item_Outlet_Sales\"]]\n#df.drop(['B', 'C'], axis=1)\nfrom sklearn.impute import KNNImputer\n\n\nimputer = KNNImputer(n_neighbors=5)\nprint(imputer.fit_transform(h))\nDF = pd.DataFrame(imputer.fit_transform(h), columns = h.columns) # We assing the new dataset with no missing values in DF\n#h[\"Item_Weight\"] = u[\"Item_Weight\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe there are no missing values in the new dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"target_col = \"Item_Outlet_Sales\"\nX = DF.loc[:, DF.columns != target_col]\ny = DF.loc[:, target_col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We split the datset into X and y. X consists of the predictor variables and y is the value we are trying to predict. In the below code, we are trying to split the values for the purpose of training and evaluating the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dataset has a lot of features(columns). We would like to narrow down the number of features so as to make our findings more intuitive and easily understandable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_regression\nfrom scipy.stats.stats import pearsonr\nout_list = []\nfor column in X_train.columns:\n    corr_tuple = pearsonr(X_train[column], y_train)\n    out_list.append([column, corr_tuple[0], corr_tuple[1]])\ncorr_df = pd.DataFrame(out_list, columns=[\"Features\", \"Correlation\", \"P-Value\"])\ncorr_df.sort_values(by=['P-Value'], inplace=True)\ncorr_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above are the features that are the most important for our dataset. Now we only keep theses predictor variables to train and test the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train[[\"Item_MRP\", \"Outlet_Type_Grocery Store\", \"Outlet_Type_Supermarket Type3\", \"Item_Visibility\", \"Outlet_Type_Supermarket Type1\"]]\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test[[\"Item_MRP\", \"Outlet_Type_Grocery Store\", \"Outlet_Type_Supermarket Type3\", \"Item_Visibility\", \"Outlet_Type_Supermarket Type1\"]]\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()  \nregressor.fit(X_train, y_train)\n#To retrieve the intercept:\nprint(regressor.intercept_)\n\n#For retrieving the slope:\nprint(regressor.coef_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is the intercept and the co-efficient of the different predictor variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coeffecients = pd.DataFrame(regressor.coef_,X_test.columns)\ncoeffecients.columns = ['Coeffecient']\ncoeffecients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above are the co-efficients of the different predictor variables in a tabular format. \n\nFrom the above table we can conclude that when the MRP of an item is increased by 1 unit then the sales of that product in a particular store increases by 15.67 units provided all other variables are kept constant. Also, if the product is in the outlet type of a grocery store then it's sales goes down by -1644.72 provided all other variables are kept constant. However, if the oulet type is supermarket type 3 then the sales of a particular product goes up by 1681.63 provided all other variables are kept constant. If the outlet type is supermarket type 1 then the sales a particular product goes up by 310.79 provided all the other variables are kept constant. If the item visibility is improved by one unit then the sales of a particular product in a store goes down by -340.03 provided all other variables remain constant. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nimport numpy as np\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we observe that the RMSE of the prediction conducted by the model is 1137.11.  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}